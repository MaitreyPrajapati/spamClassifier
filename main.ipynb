{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "VOCAB_SIZE = 400001\n",
    "DIMENSION = 300\n",
    "\n",
    "emb_dict = {}\n",
    "word_to_index = {}\n",
    "embedding_array = np.zeros((VOCAB_SIZE, DIMENSION), dtype='float32')\n",
    "\n",
    "## Loading glove embeddings 300d\n",
    "file_path = 'Embeddings/glove.6B/' \n",
    "file_name = 'glove.6B.300d.txt'\n",
    "file = io.open(file_path + file_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "\n",
    "index = 0\n",
    "\n",
    "## Loading array to pass in embedding layer and word to index layer to keep track of the words\n",
    "for line in file:\n",
    "    word, vector = line.split(' ', 1)\n",
    "    vector = list(map(float, vector.split(' ')))\n",
    "    \n",
    "    word_to_index[word.lower()] = index\n",
    "    embedding_array[index, :] = vector\n",
    "    \n",
    "    index+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing, Splitting and creating TF Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'Data/data.csv'\n",
    "\n",
    "all_data = pd.read_csv(dataPath)\n",
    "all_data['Category'] = pd.get_dummies(all_data['Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Length of the longest sentence(in words), this is needed to define the input size of the data.\n",
    "def find_max_length(all_sentences):\n",
    "    max_len = 0\n",
    "    mi = 0\n",
    "    index = 0\n",
    "    for sentence in all_sentences:\n",
    "        max_len = max(len(sentence.split(' ')), max_len)\n",
    "        if(max_len == len(sentence.split(' '))):\n",
    "            mi = index\n",
    "        index+=1\n",
    "    return max_len\n",
    "\n",
    "## Converts sentences to words indices using word_to_index dictionary, takes care of padding \n",
    "def sentences_to_indices(m, max_len, sentences, word_to_index):\n",
    "    input_array = np.zeros((m, max_len))\n",
    "    sentence_index = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split(' ')\n",
    "        words = np.array(list(map(lambda i : word_to_index[i] if i in word_to_index else word_to_index['unk'], words)))\n",
    "        words = np.pad(words, (0, max_len-words.shape[0]), constant_values = (0.))\n",
    "        \n",
    "        input_array[sentence_index, :] = words\n",
    "        sentence_index+=1\n",
    "    return input_array\n",
    "            \n",
    "## Returns embedding matrix defined earlier\n",
    "def get_embedding_matrix():\n",
    "    return embedding_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = all_data['Message']\n",
    "MAX_LEN = find_max_length(sentences)\n",
    "\n",
    "m = all_data.shape[0]\n",
    "\n",
    "input_array = sentences_to_indices(m, MAX_LEN, sentences, word_to_index)\n",
    "labels = all_data['Category'].to_numpy().reshape(m, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(input_array, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Secondary function : Returns max_len \n",
    "def get_max_len():\n",
    "    return MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.loss_ = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.loss_.append(logs.get('loss'))\n",
    "\n",
    "def NN_Model():\n",
    "    \n",
    "    def get_embedding_layer():\n",
    "        emb = keras.layers.Embedding(VOCAB_SIZE, 300, trainable=False, name='embedLayer')\n",
    "        emb.build((None,))\n",
    "        emb.set_weights([get_embedding_matrix()])\n",
    "        return emb\n",
    "    \n",
    "    input_shape = get_max_len()\n",
    "\n",
    "    input_layer = keras.layers.Input(shape = (input_shape), name='inputLayer')\n",
    "    embed_layer = get_embedding_layer()\n",
    "    \n",
    "    embedded_layer = embed_layer(input_layer)\n",
    "    \n",
    "    lstm1 = keras.layers.LSTM(128, return_sequences=True, name='lstm1')(embedded_layer)\n",
    "    dropout1 = keras.layers.Dropout(rate=0.2, name='dropout1')(lstm1)\n",
    "        \n",
    "    lstm2 = keras.layers.LSTM(128, name='lstm2')(lstm1)\n",
    "    dropout2 = keras.layers.Dropout(rate=0.2, name='dropout2')(lstm2)\n",
    "        \n",
    "    dense1 = keras.layers.Dense(5, name='dense1', activation='relu')(dropout2)\n",
    "    dense2 = keras.layers.Dense(1, name='dense2', activation='relu')(dense1)\n",
    "        \n",
    "    sigmoid = keras.layers.Activation(activation='sigmoid', name='sigmoidLayer')(dense2)\n",
    "        \n",
    "    model = keras.Model(inputs = [input_layer], outputs=[sigmoid], name='model')\n",
    "        \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_Model()\n",
    "history = AccuracyHistory()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "157/157 [==============================] - 29s 185ms/step - loss: 0.4113 - accuracy: 0.8652\n",
      "Epoch 2/3\n",
      "157/157 [==============================] - 28s 179ms/step - loss: 0.3950 - accuracy: 0.8670\n",
      "Epoch 3/3\n",
      "157/157 [==============================] - 28s 181ms/step - loss: 0.3942 - accuracy: 0.8670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a74997f10>"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, callbacks=[history], epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4113319516181946, 0.3950270414352417, 0.39417096972465515]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 143ms/step - loss: 0.4087 - accuracy: 0.8584\n"
     ]
    }
   ],
   "source": [
    "test_result = model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
